{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторная работа №2 Парсинг"
      ],
      "metadata": {
        "id": "kHV7Tz-MHjNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Теоретический материал к Лабораторной работе №2**\n",
        "\n",
        "**Тема: Основы автоматизированного сбора данных из веб-источников**\n",
        "\n",
        "#### **Введение: от веб-страницы к структурированным данным**\n",
        "\n",
        "Современный Интернет представляет собой крупнейший в истории человечества источник информации. Однако эти данные, как правило, представлены в неструктурированном, человекочитаемом формате — в виде веб-страниц. Процесс автоматического извлечения данных с веб-сайтов и их преобразования в структурированный, машиночитаемый вид (например, в таблицу или базу данных) получил название **веб-парсинг** (от англ. *to parse* — анализировать, разбирать) или **веб-скрейпинг** (*to scrape* — соскребать).\n",
        "\n",
        "Для выполнения этой задачи мы будем использовать экосистему из нескольких специализированных библиотек Python, каждая из которых выполняет свою строго определенную функцию. В данной работе мы сосредоточимся на двух основных инструментах для работы со статичными сайтами.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Библиотека `requests`: Протокол взаимодействия с веб-сервером**\n",
        "\n",
        "Любое взаимодействие в сети Интернет начинается с отправки запроса. Когда вы вводите адрес сайта в браузере, он отправляет HTTP-запрос к серверу, на котором этот сайт расположен. Сервер в ответ присылает HTML-документ, который браузер и отображает.\n",
        "\n",
        "Библиотека `requests` является отраслевым стандартом в Python для выполнения этой задачи программным путем. Её основная функция — абстрагироваться от сложностей сетевых протоколов и предоставить простой интерфейс для отправки HTTP-запросов.\n",
        "\n",
        "**Ключевые концепции и синтаксис:**\n",
        "\n",
        "1.  **Отправка GET-запроса:** Основной метод, который мы используем, — `requests.get()`. Он эмулирует переход по URL-адресу в браузере.\n",
        "\n",
        "    ```python\n",
        "    import requests\n",
        "\n",
        "    # URL-адрес целевого ресурса\n",
        "    url = 'http://quotes.toscrape.com/'\n",
        "\n",
        "    # Отправка запроса. Вся информация об ответе сервера будет храниться в объекте 'response'\n",
        "    response = requests.get(url)\n",
        "    ```\n",
        "\n",
        "2.  **Объект ответа (`response`):** Результатом вызова `requests.get()` является объект, содержащий всю информацию об ответе сервера. Наиболее важные для нас атрибуты:\n",
        "    *   `response.status_code`: Числовой код состояния HTTP. Успешный запрос возвращает код **200**. Коды, начинающиеся с 4 (например, 404 Not Found) или 5 (например, 500 Internal Server Error), свидетельствуют об ошибках. Проверка этого кода — обязательный шаг для написания надежного парсера.\n",
        "    *   `response.text`: Содержимое ответа сервера в виде текстовой строки. В нашем случае это будет полный HTML-код запрошенной страницы.\n",
        "\n",
        "    **Пример использования:**\n",
        "\n",
        "    ```python\n",
        "    if response.status_code == 200:\n",
        "        print(\"Запрос выполнен успешно.\")\n",
        "        # Получаем HTML-код страницы\n",
        "        html_content = response.text\n",
        "        print(\"Длина полученного HTML-документа:\", len(html_content), \"символов.\")\n",
        "    else:\n",
        "        print(\"Произошла ошибка при запросе. Код:\", response.status_code)\n",
        "    ```\n",
        "\n",
        "На данном этапе `requests` свою задачу выполнил: мы получили \"сырой\" HTML-документ. Далее его необходимо проанализировать.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Библиотека `BeautifulSoup`: Навигация по DOM-структуре документа**\n",
        "\n",
        "HTML-документ — это не просто текст, а строго иерархическая структура, описываемая с помощью тегов. Эту структуру принято называть **DOM-деревом** (Document Object Model). Библиотека `BeautifulSoup` является мощнейшим инструментом для парсинга этого дерева. Она преобразует текстовую строку HTML в объектную модель, по которой можно осуществлять удобную навигацию и поиск.\n",
        "\n",
        "**Ключевые концепции и синтаксис:**\n",
        "\n",
        "1.  **Инициализация объекта (\"создание супа\"):** Первым шагом является создание экземпляра класса `BeautifulSoup`, который принимает на вход HTML-текст и название парсера.\n",
        "\n",
        "    ```python\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    # html_content - это строка, полученная от requests.text\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    ```\n",
        "\n",
        "2.  **Поиск элементов:** `BeautifulSoup` предоставляет два основных метода для поиска тегов внутри DOM-дерева. Они используют **селекторы** — комбинации имени тега и его атрибутов (например, `class` или `id`).\n",
        "\n",
        "    *   **`soup.find(tag, attributes)`**: Ищет **первый** элемент, соответствующий заданным критериям, и возвращает его как объект тега. Если ничего не найдено, возвращает `None`.\n",
        "\n",
        "      **Синтаксис:**\n",
        "      ```python\n",
        "      # Поиск первого тега <h1>\n",
        "      first_h1 = soup.find('h1')\n",
        "\n",
        "      # Поиск первого тега <span> с атрибутом class='text'\n",
        "      # ВАЖНО: 'class' - зарезервированное слово в Python, поэтому используется аргумент 'class_'\n",
        "      first_quote_text = soup.find('span', class_='text')\n",
        "      ```\n",
        "\n",
        "    *   **`soup.find_all(tag, attributes)`**: Ищет **все** элементы, соответствующие критериям, и возвращает их в виде списка (`list`). Если ничего не найдено, возвращает пустой список.\n",
        "\n",
        "      **Синтаксис:**\n",
        "      ```python\n",
        "      # Поиск всех тегов <div> с атрибутом class='quote'\n",
        "      all_quote_containers = soup.find_all('div', class_='quote')\n",
        "\n",
        "      # Итерация по результатам\n",
        "      for container in all_quote_containers:\n",
        "          # Внутри каждого найденного контейнера можно продолжать поиск\n",
        "          author = container.find('small', class_='author')\n",
        "          print(author.text)\n",
        "      ```\n",
        "\n",
        "3.  **Извлечение содержимого из найденных тегов:** После того как тег найден, из него можно извлечь полезную информацию.\n",
        "    *   **`.text`**: Возвращает все текстовое содержимое внутри тега и его дочерних элементов в виде одной строки.\n",
        "    *   **`tag['attribute_name']`**: Позволяет получить значение конкретного атрибута тега. Чаще всего используется для извлечения ссылок из атрибута `href` у тега `<a>`.\n",
        "\n",
        "      **Пример использования:**\n",
        "      ```python\n",
        "      # Найдем тег с цитатой\n",
        "      quote_element = soup.find('div', class_='quote')\n",
        "\n",
        "      # Извлекаем текст цитаты\n",
        "      text = quote_element.find('span', class_='text').text\n",
        "      print(\"Текст цитаты:\", text)\n",
        "\n",
        "      # Извлекаем ссылку на автора (если она есть)\n",
        "      author_link = quote_element.find('a') # Находим первый тег <a> внутри контейнера\n",
        "      if author_link:\n",
        "          href_value = author_link['href']\n",
        "          print(\"Ссылка на страницу автора:\", href_value)\n",
        "      ```\n",
        "\n",
        "\n",
        "\n",
        "Рассмотренный ранее подход с использованием библиотек `requests` и `BeautifulSoup` является высокоэффективным для работы со **статичными** веб-страницами. \"Статичная\" страница — это документ, HTML-код которого полностью формируется на сервере и доставляется клиенту в готовом виде. Однако значительная часть современного веба функционирует иначе.\n",
        "\n",
        "**Динамические веб-сайты** активно используют технологию JavaScript для модификации своего содержимого непосредственно в браузере пользователя *после* первоначальной загрузки страницы. Это может быть подгрузка новостной ленты при прокрутке, отображение цен на авиабилеты после выбора маршрута, обновление графика погоды в реальном времени.\n",
        "\n",
        "При попытке парсинга таких сайтов с помощью `requests`, мы получим лишь базовый HTML-шаблон, в котором искомые данные будут отсутствовать, поскольку JavaScript-код, ответственный за их загрузку и отображение, не будет исполнен.\n",
        "\n",
        "Для решения этой фундаментальной проблемы необходим инструмент, который не просто запрашивает HTML, а эмулирует поведение полноценного веб-браузера. Таким инструментом является библиотека **Selenium**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Парадигма Selenium: Управление браузером вместо отправки запросов**\n",
        "\n",
        "Основное отличие Selenium от `requests` заключается в подходе. Если `requests` — это \"курьер\", доставляющий HTML-документ, то **Selenium — это \"робот-пользователь\"**, который программно запускает и управляет реальным браузером (Google Chrome, Firefox и др.).\n",
        "\n",
        "Этот подход позволяет:\n",
        "*   **Исполнять JavaScript:** Браузер под управлением Selenium загружает и выполняет все скрипты на странице.\n",
        "*   **Взаимодействовать с элементами:** Selenium может эмулировать действия пользователя, такие как клики по кнопкам, ввод текста в поля, прокрутку страницы.\n",
        "*   **Работать с итоговым HTML:** После всех динамических модификаций мы получаем доступ к финальному, \"отрисованному\" DOM-дереву, которое и видит пользователь.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Ключевые компоненты и синтаксис Selenium**\n",
        "\n",
        "##### **2.1. WebDriver: Мост между кодом и браузером**\n",
        "\n",
        "Центральным элементом Selenium является **WebDriver**. Это программный интерфейс (API), который выступает в роли \"драйвера\" или \"переводчика\" между командами в вашем Python-скрипте и действиями в реальном приложении браузера.\n",
        "\n",
        "**Инициализация WebDriver:**\n",
        "Для начала работы необходимо создать экземпляр WebDriver для конкретного браузера. Современные версии Selenium (`4.6.0` и новее) автоматически управляют загрузкой необходимого драйвера.\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "\n",
        "# Инициализация драйвера для Google Chrome.\n",
        "# Selenium сам скачает и настроит chromedriver.\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Команда ниже откроет окно браузера Chrome\n",
        "```\n",
        "\n",
        "##### **2.2. Навигация и получение страницы**\n",
        "Основной метод для загрузки страницы — `driver.get(url)`.\n",
        "\n",
        "```python\n",
        "url = 'https://www.gismeteo.ru/weather-moscow-4368/'\n",
        "driver.get(url) # Браузер откроется и перейдет по указанному адресу\n",
        "```\n",
        "\n",
        "##### **2.3. Проблема асинхронности и механизмы ожидания**\n",
        "\n",
        "Это **самая важная и сложная концепция** при работе с Selenium. Ваш Python-скрипт выполняется гораздо быстрее, чем браузер успевает загрузить страницу и выполнить все JavaScript-команды. Если вы попытаетесь найти элемент сразу после вызова `driver.get()`, скорее всего, вы получите ошибку `NoSuchElementException`, потому что элемент еще не появился на странице.\n",
        "\n",
        "**Неправильный подход:** `time.sleep(5)`. Использование жестких пауз — плохая практика. Пауза может быть слишком короткой (данные не успеют загрузиться) или слишком длинной (скрипт будет работать неэффективно).\n",
        "\n",
        "**Правильный подход: Явные ожидания (Explicit Waits)**\n",
        "Это механизм, который заставляет WebDriver ждать наступления определенного события (например, появления элемента) в течение заданного максимального времени.\n",
        "\n",
        "**Синтаксис:**\n",
        "```python\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Создаем объект ожидания: ждать максимум 10 секунд\n",
        "wait = WebDriverWait(driver, 10)\n",
        "\n",
        "# Команда \"ждать, пока элемент с указанным локатором не станет присутствовать в DOM\"\n",
        "# By.CLASS_NAME — это способ поиска (локатор)\n",
        "# 'unit_temperature_c' — значение локатора\n",
        "temperature_element = wait.until(\n",
        "    EC.presence_of_element_located((By.CLASS_NAME, 'unit_temperature_c'))\n",
        ")\n",
        "```\n",
        "\n",
        "##### **2.4. Поиск элементов и взаимодействие с ними**\n",
        "\n",
        "Для поиска элементов Selenium использует **локаторы**, которые указывают, *как* именно искать элемент. Они импортируются из `selenium.webdriver.common.by.By`.\n",
        "\n",
        "Основные локаторы:\n",
        "*   `By.ID`\n",
        "*   `By.CLASS_NAME`\n",
        "*   `By.TAG_NAME`\n",
        "*   `By.XPATH` (самый мощный и сложный)\n",
        "*   `By.CSS_SELECTOR` (часто самый удобный)\n",
        "\n",
        "**Методы поиска:**\n",
        "*   `driver.find_element(By.ЛОКАТОР, 'значение')`: Ищет **первый** элемент.\n",
        "*   `driver.find_elements(By.ЛОКАТОР, 'значение')`: Ищет **все** элементы и возвращает список.\n",
        "\n",
        "**Методы взаимодействия:**\n",
        "*   `.click()`: Кликнуть по элементу.\n",
        "*   `.send_keys('текст')`: Ввести текст в поле ввода.\n",
        "*   `.text`: Получить видимый текст элемента.\n",
        "*   `.get_attribute('атрибут')`: Получить значение атрибута (например, `href`).\n",
        "\n",
        "**Пример:**\n",
        "```python\n",
        "# Найти поле поиска по его ID\n",
        "search_box = driver.find_element(By.ID, 'search-input')\n",
        "\n",
        "# Ввести текст в поле\n",
        "search_box.send_keys('Погода в Санкт-Петербурге')\n",
        "\n",
        "# Найти и кликнуть по кнопке поиска\n",
        "search_button = driver.find_element(By.CLASS_NAME, 'search-button')\n",
        "search_button.click()\n",
        "```\n",
        "\n",
        "##### **2.5. Интеграция с BeautifulSoup и завершение работы**\n",
        "\n",
        "После того как Selenium выполнил все необходимые действия (клики, прокрутку) и дождался появления данных, мы можем получить итоговый HTML-код страницы.\n",
        "\n",
        "*   `driver.page_source`: Атрибут, содержащий финальный HTML-код страницы в виде строки.\n",
        "\n",
        "Этот код можно передать в `BeautifulSoup` для более удобного и быстрого парсинга, комбинируя сильные стороны обеих библиотек.\n",
        "\n",
        "**Обязательный шаг: Завершение сессии**\n",
        "После окончания работы необходимо закрыть браузер и завершить сессию WebDriver, чтобы освободить системные ресурсы.\n",
        "\n",
        "*   `driver.quit()`: Закрывает все окна браузера и завершает процесс WebDriver.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K4E9fULxHozb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requests"
      ],
      "metadata": {
        "id": "VuF-Ty7YmGCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json # Библиотека для работы с JSON\n",
        "\n",
        "# --- ШАГ 1: Определение цели ---\n",
        "# URL API для получения репозиториев пользователя.\n",
        "# Обратите внимание, что это не обычный URL для браузера!\n",
        "api_base_url = \"https://api.github.com/users/\"\n",
        "username = \"gvanrossum\"\n",
        "full_api_url = f\"{api_base_url}{username}/repos\"\n",
        "\n",
        "# --- ШАГ 2: Использование параметров запроса (params) ---\n",
        "# API позволяет настраивать вывод. Мы хотим отсортировать репозитории\n",
        "# по дате создания ('created') и получать по 10 штук за раз.\n",
        "# Для этого используются GET-параметры, которые requests умеет добавлять к URL.\n",
        "\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Создайте словарь 'params', который будет содержать следующие GET-параметры:\n",
        "# 1. 'sort': со значением 'created' (сортировка по дате создания)\n",
        "# 2. 'per_page': со значением '10' (выводить по 10 репозиториев на странице)\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "params = {\n",
        "    'sort': 'created',\n",
        "    'per_page': '10'\n",
        "}\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "\n",
        "# --- ШАГ 3: Выполнение запроса и обработка JSON ---\n",
        "print(f\"Отправляю запрос на: {full_api_url}\")\n",
        "response = requests.get(full_api_url, params=params)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"Запрос успешен!\")\n",
        "    # --- ЗАДАНИЕ ---\n",
        "    # Ответ от API приходит в формате JSON.\n",
        "    # У объекта response есть специальный метод .json(), который\n",
        "    # автоматически преобразует этот ответ в python-объект (список словарей).\n",
        "    # Используйте его и сохраните результат в переменную 'repos_data'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    repos_data = response.json()\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ШАГ 4: Вывод результатов ---\n",
        "    print(f\"Последние 10 созданных репозиториев пользователя {username}:\")\n",
        "    # Пройдемся циклом по списку репозиториев и выведем их названия и URL\n",
        "    for repo in repos_data:\n",
        "        print(f\"  - Название: {repo['name']}, URL: {repo['html_url']}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Ошибка! Статус-код: {response.status_code}\")\n",
        "    print(f\"Сообщение: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "RUxZBQrWLKEi",
        "outputId": "a54ca288-859c-4227-a4bf-c90f49be5b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-210946018.py, line 40)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-210946018.py\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    repos_data = response.???()\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from IPython.display import Image # Для отображения картинки в Colab\n",
        "\n",
        "# --- ШАГ 1: Найти URL изображения ---\n",
        "# Сначала нам нужно, как в прошлой лабораторной, найти ссылку на логотип.\n",
        "base_url = 'http://quotes.toscrape.com/'\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Логотип находится в теге <img> внутри тега <a> с href=\"/\".\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите тег 'img' и извлеките из него значение атрибута 'src'.\n",
        "# Сохраните относительный URL в переменную 'relative_logo_url'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "logo_element = soup.find('a', href='/').find('img')\n",
        "relative_logo_url = logo_element['src']\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "# --- ШАГ 2: Преобразование относительного URL в абсолютный ---\n",
        "# Ссылка в 'src' относительная ('/images/logo.png').\n",
        "# Чтобы ее скачать, нужен полный URL.\n",
        "absolute_logo_url = urljoin(base_url, relative_logo_url)\n",
        "print(f\"Найден абсолютный URL логотипа: {absolute_logo_url}\")\n",
        "\n",
        "\n",
        "# --- ШАГ 3: Скачивание бинарного контента ---\n",
        "print(\"Скачиваю изображение...\")\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Отправьте GET-запрос на 'absolute_logo_url'.\n",
        "# Ответ для бинарных файлов нужно получать через атрибут .content, а не .text\n",
        "# Сохраните результат в переменную 'image_content'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "image_response = requests.get('absolute_logo_url')\n",
        "if image_response.status_code == 200:\n",
        "    image_content = image_response.content\n",
        "    print(\"Изображение успешно скачано!\")\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ШАГ 4: Сохранение файла на диск ---\n",
        "    # Используем стандартный синтаксис Python для записи файлов.\n",
        "    # 'wb' означает \"write binary\" - запись в бинарном режиме.\n",
        "    file_name = 'logo.png'\n",
        "    # --- ЗАДАНИЕ ---\n",
        "    # Откройте файл 'file_name' для записи в бинарном режиме ('wb')\n",
        "    # и запишите в него 'image_content'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    with open(file_name, 'wb') as f:\n",
        "        f.write(image_content)\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "    print(f\"Файл '{file_name}' сохранен в текущую директорию Colab.\")\n",
        "\n",
        "    # Отобразим скачанное изображение прямо в блокноте\n",
        "    display(Image(file_name))\n",
        "else:\n",
        "    print(f\"Ошибка при скачивании изображения! Статус-код: {image_response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "jGRj_FXxLMwq",
        "outputId": "74b446f3-88bd-4552-95bd-2aadf23a4cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2772517807.py, line 35)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2772517807.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    image_response = requests.get(???)\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- ШАГ 1: Создание сессии и получение CSRF-токена ---\n",
        "# requests.Session() - это объект, который будет \"помнить\" cookies между запросами.\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Создайте объект сессии и сохраните его в переменную 'session'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "session = requests.Session()\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "login_url = 'http://quotes.toscrape.com/login'\n",
        "\n",
        "# Сначала делаем GET-запрос, чтобы получить страницу входа и специальный\n",
        "# \"csrf_token\" - это защита от межсайтовой подделки запроса.\n",
        "response_login_page = session.get(login_url)\n",
        "soup_login = BeautifulSoup(response_login_page.text, 'html.parser')\n",
        "\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите тег 'input' у которого атрибут name равен 'csrf_token'\n",
        "# и извлеките из него значение атрибута 'value'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "csrf_token = soup_login.find('input', {'name': 'csfr_token'})['value']\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "print(f\"Получен CSRF токен: {csrf_token}\")\n",
        "\n",
        "\n",
        "# --- ШАГ 2: Подготовка данных для POST-запроса ---\n",
        "# Это данные, которые мы бы ввели в форму на сайте.\n",
        "# Имена полей ('username', 'password') нужно посмотреть в HTML-коде страницы.\n",
        "payload = {\n",
        "    'csrf_token': csrf_token,\n",
        "    'username': 'admin',  # Используем стандартные учетные данные для этого сайта\n",
        "    'password': 'admin'\n",
        "}\n",
        "\n",
        "\n",
        "# --- ШАГ 3: Отправка POST-запроса для аутентификации ---\n",
        "# Мы отправляем POST-запрос на тот же URL, но уже с нашими данными.\n",
        "# Сессия автоматически сохранит cookies, которые вернет сервер после успешного входа.\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Отправьте POST-запрос с помощью объекта 'session'.\n",
        "# URL: login_url\n",
        "# Данные: payload\n",
        "# Сохраните ответ в 'response_post'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "response_post = session.post(login_url, data=payload)\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "\n",
        "# --- ШАГ 4: Доступ к защищенной странице ---\n",
        "# Теперь, используя ту же сессию, мы можем зайти на любую страницу сайта,\n",
        "# и сервер будет \"видеть\" нас как залогиненного пользователя.\n",
        "print(\"\\nПробую получить доступ к главной странице после логина...\")\n",
        "response_main_page = session.get('http://quotes.toscrape.com/')\n",
        "soup_main = BeautifulSoup(response_main_page.text, 'html.parser')\n",
        "\n",
        "# Проверим, видим ли мы кнопку \"Logout\"\n",
        "logout_button = soup_main.find('a', href='/logout')\n",
        "\n",
        "if logout_button:\n",
        "    print(\"Успех! Мы авторизованы. Сервер видит кнопку 'Logout'.\")\n",
        "    print(logout_button.text)\n",
        "else:\n",
        "    print(\"Неудача. Авторизация не удалась, кнопка 'Logout' не найдена.\")"
      ],
      "metadata": {
        "id": "U80X12wQLOwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NjoG6qGmmJkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BeautifulSoup4"
      ],
      "metadata": {
        "id": "a2FxGZ6QmLdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# --- ШАГ 1: Получение HTML ---\n",
        "url = 'http://books.toscrape.com/'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# --- ШАГ 2: Поиск общего контейнера ---\n",
        "# Все книги находятся внутри элементов <article> с классом 'product_pod'.\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите ВСЕ такие контейнеры с помощью .find_all() и сохраните в 'all_books'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "all_books = soup.find_all('article', class_='product_pod')\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "print(f\"Найдено книг на странице: {len(all_books)}\")\n",
        "books_data = []\n",
        "\n",
        "# --- ШАГ 3: Извлечение данных в цикле ---\n",
        "for book in all_books:\n",
        "    # --- ЗАДАНИЕ A: Найти название книги ---\n",
        "    # Название находится в теге <a> внутри тега <h3>.\n",
        "    # Нужно извлечь его атрибут 'title'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    title = book.find('a').find('h3')['title']\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ЗАДАНИЕ B: Найти цену ---\n",
        "    # Цена находится в теге <p> с классом 'price_color'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    price = book.find('p', class_='price_color').text\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ЗАДАНИЕ C: Найти рейтинг ---\n",
        "    # Рейтинг находится в атрибуте 'class' у тега <p>, который начинается с 'star-rating'.\n",
        "    # Например: <p class=\"star-rating Three\">. Нам нужно извлечь слово 'Three'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    rating_container = book.find('p', class_='star-rating')\n",
        "    # Атрибут 'class' возвращает список классов, например, ['star-rating', 'Three']\n",
        "    rating = rating_container['class'][1] # Берем второй элемент\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # Добавляем собранные данные в наш список\n",
        "    books_data.append({\n",
        "        'Title': title,\n",
        "        'Price': price,\n",
        "        'Rating': rating\n",
        "    })\n",
        "\n",
        "# --- ШАГ 4: Вывод результата ---\n",
        "df = pd.DataFrame(books_data)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "BBNhpZgPMGUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# --- ШАГ 1: Получение HTML ---\n",
        "url = 'https://ru.wikipedia.org/wiki/Python'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# --- ШАГ 2: Поиск таблицы-инфобокса ---\n",
        "# Инфобокс - это таблица с классом 'infobox'.\n",
        "infobox = soup.find('table', class_='infobox')\n",
        "\n",
        "# --- ШАГ 3: Поиск всех строк с данными ---\n",
        "# Все строки в таблице - это теги <tr>.\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите все теги <tr> внутри 'infobox'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "rows = infobox.find_all('tr')\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "infobox_data = []\n",
        "\n",
        "# --- ШАГ 4: Навигация по соседним элементам в цикле ---\n",
        "for row in rows:\n",
        "    # Нас интересуют строки, где есть и заголовок (<th>) и значение (<td>)\n",
        "    header = row.find('th')\n",
        "    data_cell = row.find('td')\n",
        "\n",
        "    if header and data_cell:\n",
        "        # --- ЗАДАНИЕ ---\n",
        "        # Извлеките текст из ячейки-заголовка 'header'.\n",
        "        # Извлеките текст из ячейки-данных 'data_cell'.\n",
        "        # Используйте .text.strip() для удаления лишних пробелов и переносов строк.\n",
        "        # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "        key = header.text.strip()\n",
        "        value = data_cell.text.strip()\n",
        "        # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "        infobox_data.append({\n",
        "            'Характеристика': key,\n",
        "            'Значение': value\n",
        "        })\n",
        "\n",
        "# --- ШАГ 5: Вывод результата ---\n",
        "df = pd.DataFrame(infobox_data)\n",
        "display(df)\n",
        "```**Чему вы научились:**\n",
        "1.  Искать вложенные элементы (`infobox.find_all('tr')`).\n",
        "2.  **Навигации по горизонтали:** находить \"соседние\" теги (`<th>` и `<td>`) внутри общего родителя (`<tr>`).\n",
        "3.  Использовать `.strip()` для базовой очистки текстовых данных.\n",
        "\n",
        "---\n",
        "### **Задача 3: \"Дата-клинер\" — Очистка и преобразование данных**\n",
        "\n",
        "**Контекст:** Реальные данные почти всегда \"грязные\". Цены содержат символы валют, текст — лишние пробелы и переносы, а числа хранятся в виде строк. Профессиональный парсинг включает в себя не только извлечение, но и очистку.\n",
        "\n",
        "**Ваша миссия:** Перейти на страницу первой книги из каталога, извлечь из нее цену и описание продукта, а затем \"очистить\" их: цену превратить в число, а из описания убрать лишние символы.\n",
        "\n",
        "**(Ячейка 3: Код)**\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import re # Библиотека для работы с регулярными выражениями\n",
        "\n",
        "# --- ШАГ 1: Получаем URL первой книги ---\n",
        "base_url = 'http://books.toscrape.com/catalogue/' # Важно: для urljoin нужен правильный базовый путь\n",
        "main_page_url = 'http://books.toscrape.com/'\n",
        "response = requests.get(main_page_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Находим ссылку на первую книгу\n",
        "first_book_link = soup.find('h3').find('a')['href']\n",
        "book_url = urljoin(base_url, first_book_link)\n",
        "print(f\"URL страницы книги: {book_url}\")\n",
        "\n",
        "# --- ШАГ 2: Переходим на страницу книги и создаем новый \"суп\" ---\n",
        "response_book = requests.get(book_url)\n",
        "soup_book = BeautifulSoup(response_book.text, 'html.parser')\n",
        "\n",
        "\n",
        "# --- ШАГ 3: Извлечение и очистка ЦЕНЫ ---\n",
        "price_raw = soup_book.find('p', class_='price_color').text\n",
        "print(f\"Сырая строка с ценой: {price_raw}\") # Например, '£51.77'\n",
        "\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Напишите код для очистки цены.\n",
        "# Нужно убрать все символы, кроме цифр и точки, а затем превратить строку в число (float).\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "# re.sub(r'[^\\d.]', '', price_raw) - эта команда удалит все, что не является цифрой (\\d) или точкой (.).\n",
        "price_clean_str = re.sub(r'[^\\d.]', '', price_raw)\n",
        "price_float = float(price_clean_str)\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "print(f\"Очищенная цена (число): {price_float}\")\n",
        "print(f\"Тип данных цены: {type(price_float)}\")\n",
        "\n",
        "\n",
        "# --- ШАГ 4: Извлечение и очистка ОПИСАНИЯ ---\n",
        "# Описание находится в теге <p> сразу ПОСЛЕ тега <div> с id='product_description'.\n",
        "# Это идеальный случай для использования .find_next_sibling()\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите тег <div> с id 'product_description', а затем\n",
        "# с помощью .find_next_sibling('p') найдите следующий за ним тег <p>.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "description_header = soup_book.find('div', id='product_description')\n",
        "description_raw = description_header.find_next_sibling('p').text\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "print(f\"\\nСырое описание:\\n {description_raw[:150]}...\")\n",
        "\n",
        "# Очищаем описание от лишних пробелов и символов\n",
        "description_clean = description_raw.strip()\n",
        "print(f\"\\nОчищенное описание:\\n {description_clean[:150]}...\")"
      ],
      "metadata": {
        "id": "3yh3mDG1MJPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# --- ШАГ 1: Получение HTML ---\n",
        "url = 'https://ru.wikipedia.org/wiki/Python'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# --- ШАГ 2: Поиск таблицы-инфобокса ---\n",
        "# Инфобокс - это таблица с классом 'infobox'.\n",
        "infobox = soup.find('table', class_='infobox')\n",
        "\n",
        "# --- ШАГ 3: Поиск всех строк с данными ---\n",
        "# Все строки в таблице - это теги <tr>.\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите все теги <tr> внутри 'infobox'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "rows = infobox.find_all('tr')\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "infobox_data = []\n",
        "\n",
        "# --- ШАГ 4: Навигация по соседним элементам в цикле ---\n",
        "for row in rows:\n",
        "    # Нас интересуют строки, где есть и заголовок (<th>) и значение (<td>)\n",
        "    header = row.find('th')\n",
        "    data_cell = row.find('td')\n",
        "\n",
        "    if header and data_cell:\n",
        "        # --- ЗАДАНИЕ ---\n",
        "        # Извлеките текст из ячейки-заголовка 'header'.\n",
        "        # Извлеките текст из ячейки-данных 'data_cell'.\n",
        "        # Используйте .text.strip() для удаления лишних пробелов и переносов строк.\n",
        "        # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "        key = header.text.strip()\n",
        "        value = data_cell.text.strip()\n",
        "        # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "        infobox_data.append({\n",
        "            'Характеристика': key,\n",
        "            'Значение': value\n",
        "        })\n",
        "\n",
        "# --- ШАГ 5: Вывод результата ---\n",
        "df = pd.DataFrame(infobox_data)\n",
        "display(df)\n",
        "```**Чему вы научились:**\n",
        "1.  Искать вложенные элементы (`infobox.find_all('tr')`).\n",
        "2.  **Навигации по горизонтали:** находить \"соседние\" теги (`<th>` и `<td>`) внутри общего родителя (`<tr>`).\n",
        "3.  Использовать `.strip()` для базовой очистки текстовых данных.\n",
        "\n",
        "---\n",
        "### **Задача 3: \"Дата-клинер\" — Очистка и преобразование данных**\n",
        "\n",
        "**Контекст:** Реальные данные почти всегда \"грязные\". Цены содержат символы валют, текст — лишние пробелы и переносы, а числа хранятся в виде строк. Профессиональный парсинг включает в себя не только извлечение, но и очистку.\n",
        "\n",
        "**Ваша миссия:** Перейти на страницу первой книги из каталога, извлечь из нее цену и описание продукта, а затем \"очистить\" их: цену превратить в число, а из описания убрать лишние символы.\n",
        "\n",
        "**(Ячейка 3: Код)**\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import re # Библиотека для работы с регулярными выражениями\n",
        "\n",
        "# --- ШАГ 1: Получаем URL первой книги ---\n",
        "base_url = 'http://books.toscrape.com/catalogue/' # Важно: для urljoin нужен правильный базовый путь\n",
        "main_page_url = 'http://books.toscrape.com/'\n",
        "response = requests.get(main_page_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Находим ссылку на первую книгу\n",
        "first_book_link = soup.find('h3').find('a')['href']\n",
        "book_url = urljoin(base_url, first_book_link)\n",
        "print(f\"URL страницы книги: {book_url}\")\n",
        "\n",
        "# --- ШАГ 2: Переходим на страницу книги и создаем новый \"суп\" ---\n",
        "response_book = requests.get(book_url)\n",
        "soup_book = BeautifulSoup(response_book.text, 'html.parser')\n",
        "\n",
        "\n",
        "# --- ШАГ 3: Извлечение и очистка ЦЕНЫ ---\n",
        "price_raw = soup_book.find('p', class_='price_color').text\n",
        "print(f\"Сырая строка с ценой: {price_raw}\") # Например, '£51.77'\n",
        "\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Напишите код для очистки цены.\n",
        "# Нужно убрать все символы, кроме цифр и точки, а затем превратить строку в число (float).\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "# re.sub(r'[^\\d.]', '', price_raw) - эта команда удалит все, что не является цифрой (\\d) или точкой (.).\n",
        "price_clean_str = re.sub(r'[^\\d.]', '', price_raw)\n",
        "price_float = float(price_clean_str)\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "print(f\"Очищенная цена (число): {price_float}\")\n",
        "print(f\"Тип данных цены: {type(price_float)}\")\n",
        "\n",
        "\n",
        "# --- ШАГ 4: Извлечение и очистка ОПИСАНИЯ ---\n",
        "# Описание находится в теге <p> сразу ПОСЛЕ тега <div> с id='product_description'.\n",
        "# Это идеальный случай для использования .find_next_sibling()\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите тег <div> с id 'product_description', а затем\n",
        "# с помощью .find_next_sibling('p') найдите следующий за ним тег <p>.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "description_header = soup_book.find('div', id='product_description')\n",
        "description_raw = description_header.find_next_sibling('p').text\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "print(f\"\\nСырое описание:\\n {description_raw[:150]}...\")\n",
        "\n",
        "# Очищаем описание от лишних пробелов и символов\n",
        "description_clean = description_raw.strip()\n",
        "print(f\"\\nОчищенное описание:\\n {description_clean[:150]}...\")"
      ],
      "metadata": {
        "id": "kcXoGYQOMMEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "SmYV2Vwj7hIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selenium"
      ],
      "metadata": {
        "id": "OvMnn8-LmPtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Настройка selenium"
      ],
      "metadata": {
        "id": "9RER97g1NSEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка Selenium и webdriver-manager для автоматического управления драйверами\n",
        "!pip install selenium webdriver-manager\n",
        "\n",
        "# Настройка опций для запуска Chrome в \"безголовом\" режиме (без GUI) в среде Colab\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# --- Этот код нужно будет использовать в каждой задаче для инициализации драйвера ---\n",
        "def setup_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless') # Запуск без открытия окна браузера\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
        "    return driver\n",
        "# -----------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "rU4wrDXFNUCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "\n",
        "# --- ШАГ 1: Инициализация драйвера и переход на страницу ---\n",
        "driver = setup_driver()\n",
        "url = 'http://quotes.toscrape.com/scroll'\n",
        "driver.get(url)\n",
        "print(f\"Перешел на страницу: {url}\")\n",
        "\n",
        "\n",
        "# --- ШАГ 2: Логика \"бесконечной\" прокрутки ---\n",
        "# Мы будем прокручивать страницу вниз, пока количество цитат не перестанет увеличиваться.\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "all_quotes_texts = set() # Используем set для автоматического удаления дубликатов\n",
        "\n",
        "while True:\n",
        "    # --- ЗАДАНИЕ A: Собрать цитаты, видимые на данный момент ---\n",
        "    # Найдите все элементы с классом 'quote' и добавьте их текст в 'all_quotes_texts'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    quotes_elements = driver.find_elements(By.CLASS_NAME, 'quote')\n",
        "    for quote in quotes_elements:\n",
        "        all_quotes_texts.add(quote.find_element(By.CLASS_NAME, 'quote').text)\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ЗАДАНИЕ B: Прокрутить страницу до самого низа ---\n",
        "    # Используйте driver.execute_script() для выполнения JavaScript-кода.\n",
        "    # Команда \"window.scrollTo(0, document.body.scrollHeight);\" прокручивает страницу вниз.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # Даем странице время на подгрузку нового контента\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    # --- ШАГ 3: Проверка условия выхода из цикла ---\n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        print(\"Достигнут конец страницы. Выхожу из цикла.\")\n",
        "        break # Выходим, если высота страницы больше не меняется\n",
        "    last_height = new_height\n",
        "\n",
        "# --- ШАГ 4: Вывод результата и закрытие браузера ---\n",
        "print(f\"\\nСобрано уникальных цитат: {len(all_quotes_texts)}\")\n",
        "print(\"Пример одной из собранных цитат:\")\n",
        "print(list(all_quotes_texts)[0])\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "OZgkTcm-NW3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "# --- ШАГ 1: Инициализация и переход на сайт ---\n",
        "driver = setup_driver()\n",
        "# ВАЖНО: Структура и селекторы на живых сайтах могут меняться!\n",
        "# Этот код актуален на момент написания.\n",
        "url = 'https://www.aviasales.ru/'\n",
        "driver.get(url)\n",
        "print(f\"Перешел на страницу: {url}\")\n",
        "time.sleep(2) # Небольшая пауза, чтобы страница прогрузилась\n",
        "\n",
        "# --- ШАГ 2: Ввод данных в форму поиска ---\n",
        "try:\n",
        "    # --- ЗАДАНИЕ A: Ввести город отправления ---\n",
        "    # Найдите поле ввода для города отправления (id='origin').\n",
        "    # Очистите его (.clear()) и введите \"Москва\" (.send_keys()).\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    origin = driver.find_element(By.ID, 'origin')\n",
        "    origin.clear()\n",
        "    origin.send_keys(\"Москва\")\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "    time.sleep(1) # Пауза, чтобы появился выпадающий список\n",
        "\n",
        "    # --- ЗАДАНИЕ B: Ввести город назначения ---\n",
        "    # По аналогии найдите поле назначения (id='destination') и введите \"Санкт-Петербург\".\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    destination = driver.find_element(By.ID, 'destination')\n",
        "    destination.clear()\n",
        "    destination.send_keys(\"Санкт-Петербург\")\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "    time.sleep(1)\n",
        "\n",
        "    # --- ЗАДАНИЕ C: Нажать на кнопку поиска ---\n",
        "    # Кнопка поиска часто имеет сложный селектор.\n",
        "    # Будем искать по тексту \"Найти билеты\" внутри тега <div>\n",
        "    # XPath-селектор для этого: \"//div[text()='Найти билеты']\"\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    search_button = driver.find_element(By.XPATH, \"//div[text()='Найти билеты']\")\n",
        "    search_button.click()\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ШАГ 3: Ожидание и извлечение результатов ---\n",
        "    print(\"Нажал 'Найти'. Жду появления результатов...\")\n",
        "    # --- ЗАДАНИЕ D: Дождаться загрузки первого билета ---\n",
        "    # Это ключевой шаг. Мы ждем до 20 секунд, пока на странице не появится\n",
        "    # элемент, сигнализирующий о загрузке результатов (например, контейнер с ценой).\n",
        "    # Используем WebDriverWait и Expected Conditions (EC).\n",
        "    # Ищем первый элемент с атрибутом 'data-test-id=\"ticket-card\"'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    wait = WebDriverWait(driver, 20)\n",
        "    first_ticket = wait.until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-test-id='ticket-card']\"))\n",
        "    )\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    print(\"Результаты загружены!\")\n",
        "    # Извлекаем цену из первого найденного билета\n",
        "    price = first_ticket.find_element(By.CSS_SELECTOR, \"[data-test-id='price']\").text\n",
        "    print(f\"Цена первого найденного билета: {price}\")\n",
        "\n",
        "finally:\n",
        "    # --- ШАГ 4: Обязательное закрытие браузера ---\n",
        "    driver.quit()"
      ],
      "metadata": {
        "id": "viUnYMXmNZwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Контрольные вопросы по библиотекам для парсинга**\n",
        "\n",
        "#### **Блок 1: Библиотека `requests` (\"Курьер\")**\n",
        "\n",
        "Эти вопросы проверяют ваше умение получать данные с сервера.\n",
        "\n",
        "1.  **Фундаментальный вопрос:** В чем основная задача библиотеки `requests`? Что она делает, а чего, наоборот, делать **не умеет** (например, в контексте JavaScript)?\n",
        "2.  **Типы запросов:** В чем принципиальная разница между `GET` и `POST` запросами? В какой из наших лабораторных задач мы использовали `POST` и для какой цели?\n",
        "3.  **Объект ответа:** Вы выполнили команду `response = requests.get(url)`. Какие три важнейших атрибута объекта `response` вы будете использовать и для чего каждый из них предназначен (`status_code`, `text`, `content`)?\n",
        "4.  **Обработка ошибок:** Почему проверка `response.status_code == 200` является обязательным шагом в любом надежном парсере? Что означает код `404`? А код `403`?\n",
        "5.  **Работа с API:** Почему для получения данных от API (как в задаче с GitHub) мы использовали метод `response.json()`, а не просто брали `response.text`? В чем преимущество такого подхода?\n",
        "6.  **Сессии:** Объясните своими словами, что такое `requests.Session()`. Какую проблему решает объект сессии, и почему без него не удалось бы выполнить задачу с авторизацией на сайте?\n",
        "7.  **Параметры запроса:** Как с помощью `requests` передать в URL GET-параметры (например, `?sort=date&page=2`) без ручного формирования строки URL? Какой аргумент функции `get()` для этого используется?\n",
        "\n",
        "---\n",
        "\n",
        "#### **Блок 2: Библиотека `BeautifulSoup` (\"Навигатор\")**\n",
        "\n",
        "Эти вопросы проверяют ваше умение разбирать HTML-код и находить в нем нужные данные.\n",
        "\n",
        "8.  **Основное назначение:** Какую проблему решает `BeautifulSoup`? Что она принимает на вход и что отдает на выходе?\n",
        "9.  **Ключевое различие:** В чем фундаментальная разница между методами `.find()` и `.find_all()`? Приведите пример, когда нужно использовать один, а когда — другой.\n",
        "10. **Синтаксис поиска:** Как найти тег `<p>` с CSS-классом `price_color`? Почему в коде мы пишем `class_` с нижним подчеркиванием, а не просто `class`?\n",
        "11. **Извлечение данных:** У вас есть объект тега, сохраненный в переменной `tag`. Как из него извлечь:\n",
        "    *   Весь видимый текст внутри него?\n",
        "    *   Значение атрибута `href`?\n",
        "12. **Вложенный поиск:** Ваш парсер нашел общий контейнер товара (`<div class=\"product\">`). Как продолжить поиск и найти цену, которая находится **внутри** этого контейнера? Напишите примерный код.\n",
        "13. **Продвинутая навигация:** Представьте, что вы нашли заголовок `<h2>Описание</h2>`. Само описание находится в следующем за ним теге `<p>`. Какой метод `BeautifulSoup` позволит вам найти этот \"соседний\" тег, не начиная поиск заново от корня документа?\n",
        "14. **Очистка данных:** Почему простого извлечения `.text` часто недостаточно для реальных задач? Какие две стандартные операции по очистке текста вы применяли в лабораторных работах?\n",
        "\n",
        "---\n",
        "\n",
        "#### **Блок 3: Библиотека `Selenium` (\"Робот-пользователь\")**\n",
        "\n",
        "Эти вопросы проверяют ваше понимание работы с динамическими сайтами и автоматизацией браузера.\n",
        "\n",
        "15. **Главный вопрос:** Назовите основную причину, по которой мы вынуждены использовать `Selenium`, а не `requests`. Какую технологию `Selenium` умеет обрабатывать, а `requests` — нет?\n",
        "16. **Проблема синхронизации:** Почему использование `time.sleep()` для ожидания загрузки элементов на странице является плохой практикой? Каков правильный, надежный способ дождаться появления элемента?\n",
        "17. **Явные ожидания (Explicit Waits):** Объясните своими словами, что делают эти три строки кода:\n",
        "    ```python\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "    element = wait.until(\n",
        "        EC.presence_of_element_located((By.CLASS_NAME, 'price'))\n",
        "    )\n",
        "    ```\n",
        "18. **Взаимодействие с формами:** Опишите последовательность из трех основных действий, которые нужно совершить с помощью `Selenium`, чтобы ввести текст в поле поиска и нажать на кнопку.\n",
        "19. **Исполнение JavaScript:** Для чего в задаче с \"бесконечным свитком\" мы использовали команду `driver.execute_script()`? Можно ли было добиться того же результата другим методом `Selenium`?\n",
        "20. **Интеграция библиотек:** В какой момент работы парсера на `Selenium` имеет смысл передать управление библиотеке `BeautifulSoup`? Что для этого нужно получить от `driver` и как это сделать?\n",
        "21. **Завершение работы:** Почему команда `driver.quit()` является обязательной в конце скрипта? Что произойдет, если ее не вызывать?\n",
        "\n",
        "---\n",
        "\n",
        "#### **Блок 4: Синтез и сценарии (Проверка общего понимания)**\n",
        "\n",
        "22. **Выбор инструмента:** Вам нужно спарсить три сайта:\n",
        "    *   А) Таблицу курсов валют со страницы Центробанка.\n",
        "    *   Б) Ленту комментариев на YouTube, которая подгружается при прокрутке.\n",
        "    *   В) Данные о погоде с публичного погодного API.\n",
        "    Какой основной инструмент (`requests`, `bs4`, `Selenium`) вы выберете для **каждой** из этих задач и почему?\n",
        "\n",
        "23. **Отладка:** Ваш парсер на `BeautifulSoup` вчера работал, а сегодня перестал, выдавая ошибку `AttributeError: 'NoneType' object has no attribute 'text'`. Назовите самую вероятную причину этой проблемы. Каков ваш первый шаг для диагностики?\n",
        "\n",
        "24. **Этика парсинга:** Что такое файл `robots.txt` на сайте и почему его рекомендуется проверять перед запуском массового сбора данных?"
      ],
      "metadata": {
        "id": "MorSgT4YmN_A"
      }
    }
  ]
}