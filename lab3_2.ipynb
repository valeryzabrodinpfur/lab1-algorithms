{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHV7Tz-MHjNG"
   },
   "source": [
    "# Лабораторная работа №2 Парсинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4E9fULxHozb"
   },
   "source": [
    "\n",
    "### **Теоретический материал к Лабораторной работе №2**\n",
    "\n",
    "**Тема: Основы автоматизированного сбора данных из веб-источников**\n",
    "\n",
    "#### **Введение: от веб-страницы к структурированным данным**\n",
    "\n",
    "Современный Интернет представляет собой крупнейший в истории человечества источник информации. Однако эти данные, как правило, представлены в неструктурированном, человекочитаемом формате — в виде веб-страниц. Процесс автоматического извлечения данных с веб-сайтов и их преобразования в структурированный, машиночитаемый вид (например, в таблицу или базу данных) получил название **веб-парсинг** (от англ. *to parse* — анализировать, разбирать) или **веб-скрейпинг** (*to scrape* — соскребать).\n",
    "\n",
    "Для выполнения этой задачи мы будем использовать экосистему из нескольких специализированных библиотек Python, каждая из которых выполняет свою строго определенную функцию. В данной работе мы сосредоточимся на двух основных инструментах для работы со статичными сайтами.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Библиотека `requests`: Протокол взаимодействия с веб-сервером**\n",
    "\n",
    "Любое взаимодействие в сети Интернет начинается с отправки запроса. Когда вы вводите адрес сайта в браузере, он отправляет HTTP-запрос к серверу, на котором этот сайт расположен. Сервер в ответ присылает HTML-документ, который браузер и отображает.\n",
    "\n",
    "Библиотека `requests` является отраслевым стандартом в Python для выполнения этой задачи программным путем. Её основная функция — абстрагироваться от сложностей сетевых протоколов и предоставить простой интерфейс для отправки HTTP-запросов.\n",
    "\n",
    "**Ключевые концепции и синтаксис:**\n",
    "\n",
    "1.  **Отправка GET-запроса:** Основной метод, который мы используем, — `requests.get()`. Он эмулирует переход по URL-адресу в браузере.\n",
    "\n",
    "    ```python\n",
    "    import requests\n",
    "\n",
    "    # URL-адрес целевого ресурса\n",
    "    url = 'http://quotes.toscrape.com/'\n",
    "\n",
    "    # Отправка запроса. Вся информация об ответе сервера будет храниться в объекте 'response'\n",
    "    response = requests.get(url)\n",
    "    ```\n",
    "\n",
    "2.  **Объект ответа (`response`):** Результатом вызова `requests.get()` является объект, содержащий всю информацию об ответе сервера. Наиболее важные для нас атрибуты:\n",
    "    *   `response.status_code`: Числовой код состояния HTTP. Успешный запрос возвращает код **200**. Коды, начинающиеся с 4 (например, 404 Not Found) или 5 (например, 500 Internal Server Error), свидетельствуют об ошибках. Проверка этого кода — обязательный шаг для написания надежного парсера.\n",
    "    *   `response.text`: Содержимое ответа сервера в виде текстовой строки. В нашем случае это будет полный HTML-код запрошенной страницы.\n",
    "\n",
    "    **Пример использования:**\n",
    "\n",
    "    ```python\n",
    "    if response.status_code == 200:\n",
    "        print(\"Запрос выполнен успешно.\")\n",
    "        # Получаем HTML-код страницы\n",
    "        html_content = response.text\n",
    "        print(\"Длина полученного HTML-документа:\", len(html_content), \"символов.\")\n",
    "    else:\n",
    "        print(\"Произошла ошибка при запросе. Код:\", response.status_code)\n",
    "    ```\n",
    "\n",
    "На данном этапе `requests` свою задачу выполнил: мы получили \"сырой\" HTML-документ. Далее его необходимо проанализировать.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Библиотека `BeautifulSoup`: Навигация по DOM-структуре документа**\n",
    "\n",
    "HTML-документ — это не просто текст, а строго иерархическая структура, описываемая с помощью тегов. Эту структуру принято называть **DOM-деревом** (Document Object Model). Библиотека `BeautifulSoup` является мощнейшим инструментом для парсинга этого дерева. Она преобразует текстовую строку HTML в объектную модель, по которой можно осуществлять удобную навигацию и поиск.\n",
    "\n",
    "**Ключевые концепции и синтаксис:**\n",
    "\n",
    "1.  **Инициализация объекта (\"создание супа\"):** Первым шагом является создание экземпляра класса `BeautifulSoup`, который принимает на вход HTML-текст и название парсера.\n",
    "\n",
    "    ```python\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # html_content - это строка, полученная от requests.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    ```\n",
    "\n",
    "2.  **Поиск элементов:** `BeautifulSoup` предоставляет два основных метода для поиска тегов внутри DOM-дерева. Они используют **селекторы** — комбинации имени тега и его атрибутов (например, `class` или `id`).\n",
    "\n",
    "    *   **`soup.find(tag, attributes)`**: Ищет **первый** элемент, соответствующий заданным критериям, и возвращает его как объект тега. Если ничего не найдено, возвращает `None`.\n",
    "\n",
    "      **Синтаксис:**\n",
    "      ```python\n",
    "      # Поиск первого тега <h1>\n",
    "      first_h1 = soup.find('h1')\n",
    "\n",
    "      # Поиск первого тега <span> с атрибутом class='text'\n",
    "      # ВАЖНО: 'class' - зарезервированное слово в Python, поэтому используется аргумент 'class_'\n",
    "      first_quote_text = soup.find('span', class_='text')\n",
    "      ```\n",
    "\n",
    "    *   **`soup.find_all(tag, attributes)`**: Ищет **все** элементы, соответствующие критериям, и возвращает их в виде списка (`list`). Если ничего не найдено, возвращает пустой список.\n",
    "\n",
    "      **Синтаксис:**\n",
    "      ```python\n",
    "      # Поиск всех тегов <div> с атрибутом class='quote'\n",
    "      all_quote_containers = soup.find_all('div', class_='quote')\n",
    "\n",
    "      # Итерация по результатам\n",
    "      for container in all_quote_containers:\n",
    "          # Внутри каждого найденного контейнера можно продолжать поиск\n",
    "          author = container.find('small', class_='author')\n",
    "          print(author.text)\n",
    "      ```\n",
    "\n",
    "3.  **Извлечение содержимого из найденных тегов:** После того как тег найден, из него можно извлечь полезную информацию.\n",
    "    *   **`.text`**: Возвращает все текстовое содержимое внутри тега и его дочерних элементов в виде одной строки.\n",
    "    *   **`tag['attribute_name']`**: Позволяет получить значение конкретного атрибута тега. Чаще всего используется для извлечения ссылок из атрибута `href` у тега `<a>`.\n",
    "\n",
    "      **Пример использования:**\n",
    "      ```python\n",
    "      # Найдем тег с цитатой\n",
    "      quote_element = soup.find('div', class_='quote')\n",
    "\n",
    "      # Извлекаем текст цитаты\n",
    "      text = quote_element.find('span', class_='text').text\n",
    "      print(\"Текст цитаты:\", text)\n",
    "\n",
    "      # Извлекаем ссылку на автора (если она есть)\n",
    "      author_link = quote_element.find('a') # Находим первый тег <a> внутри контейнера\n",
    "      if author_link:\n",
    "          href_value = author_link['href']\n",
    "          print(\"Ссылка на страницу автора:\", href_value)\n",
    "      ```\n",
    "\n",
    "\n",
    "\n",
    "Рассмотренный ранее подход с использованием библиотек `requests` и `BeautifulSoup` является высокоэффективным для работы со **статичными** веб-страницами. \"Статичная\" страница — это документ, HTML-код которого полностью формируется на сервере и доставляется клиенту в готовом виде. Однако значительная часть современного веба функционирует иначе.\n",
    "\n",
    "**Динамические веб-сайты** активно используют технологию JavaScript для модификации своего содержимого непосредственно в браузере пользователя *после* первоначальной загрузки страницы. Это может быть подгрузка новостной ленты при прокрутке, отображение цен на авиабилеты после выбора маршрута, обновление графика погоды в реальном времени.\n",
    "\n",
    "При попытке парсинга таких сайтов с помощью `requests`, мы получим лишь базовый HTML-шаблон, в котором искомые данные будут отсутствовать, поскольку JavaScript-код, ответственный за их загрузку и отображение, не будет исполнен.\n",
    "\n",
    "Для решения этой фундаментальной проблемы необходим инструмент, который не просто запрашивает HTML, а эмулирует поведение полноценного веб-браузера. Таким инструментом является библиотека **Selenium**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Парадигма Selenium: Управление браузером вместо отправки запросов**\n",
    "\n",
    "Основное отличие Selenium от `requests` заключается в подходе. Если `requests` — это \"курьер\", доставляющий HTML-документ, то **Selenium — это \"робот-пользователь\"**, который программно запускает и управляет реальным браузером (Google Chrome, Firefox и др.).\n",
    "\n",
    "Этот подход позволяет:\n",
    "*   **Исполнять JavaScript:** Браузер под управлением Selenium загружает и выполняет все скрипты на странице.\n",
    "*   **Взаимодействовать с элементами:** Selenium может эмулировать действия пользователя, такие как клики по кнопкам, ввод текста в поля, прокрутку страницы.\n",
    "*   **Работать с итоговым HTML:** После всех динамических модификаций мы получаем доступ к финальному, \"отрисованному\" DOM-дереву, которое и видит пользователь.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Ключевые компоненты и синтаксис Selenium**\n",
    "\n",
    "##### **2.1. WebDriver: Мост между кодом и браузером**\n",
    "\n",
    "Центральным элементом Selenium является **WebDriver**. Это программный интерфейс (API), который выступает в роли \"драйвера\" или \"переводчика\" между командами в вашем Python-скрипте и действиями в реальном приложении браузера.\n",
    "\n",
    "**Инициализация WebDriver:**\n",
    "Для начала работы необходимо создать экземпляр WebDriver для конкретного браузера. Современные версии Selenium (`4.6.0` и новее) автоматически управляют загрузкой необходимого драйвера.\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "\n",
    "# Инициализация драйвера для Google Chrome.\n",
    "# Selenium сам скачает и настроит chromedriver.\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Команда ниже откроет окно браузера Chrome\n",
    "```\n",
    "\n",
    "##### **2.2. Навигация и получение страницы**\n",
    "Основной метод для загрузки страницы — `driver.get(url)`.\n",
    "\n",
    "```python\n",
    "url = 'https://www.gismeteo.ru/weather-moscow-4368/'\n",
    "driver.get(url) # Браузер откроется и перейдет по указанному адресу\n",
    "```\n",
    "\n",
    "##### **2.3. Проблема асинхронности и механизмы ожидания**\n",
    "\n",
    "Это **самая важная и сложная концепция** при работе с Selenium. Ваш Python-скрипт выполняется гораздо быстрее, чем браузер успевает загрузить страницу и выполнить все JavaScript-команды. Если вы попытаетесь найти элемент сразу после вызова `driver.get()`, скорее всего, вы получите ошибку `NoSuchElementException`, потому что элемент еще не появился на странице.\n",
    "\n",
    "**Неправильный подход:** `time.sleep(5)`. Использование жестких пауз — плохая практика. Пауза может быть слишком короткой (данные не успеют загрузиться) или слишком длинной (скрипт будет работать неэффективно).\n",
    "\n",
    "**Правильный подход: Явные ожидания (Explicit Waits)**\n",
    "Это механизм, который заставляет WebDriver ждать наступления определенного события (например, появления элемента) в течение заданного максимального времени.\n",
    "\n",
    "**Синтаксис:**\n",
    "```python\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Создаем объект ожидания: ждать максимум 10 секунд\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Команда \"ждать, пока элемент с указанным локатором не станет присутствовать в DOM\"\n",
    "# By.CLASS_NAME — это способ поиска (локатор)\n",
    "# 'unit_temperature_c' — значение локатора\n",
    "temperature_element = wait.until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, 'unit_temperature_c'))\n",
    ")\n",
    "```\n",
    "\n",
    "##### **2.4. Поиск элементов и взаимодействие с ними**\n",
    "\n",
    "Для поиска элементов Selenium использует **локаторы**, которые указывают, *как* именно искать элемент. Они импортируются из `selenium.webdriver.common.by.By`.\n",
    "\n",
    "Основные локаторы:\n",
    "*   `By.ID`\n",
    "*   `By.CLASS_NAME`\n",
    "*   `By.TAG_NAME`\n",
    "*   `By.XPATH` (самый мощный и сложный)\n",
    "*   `By.CSS_SELECTOR` (часто самый удобный)\n",
    "\n",
    "**Методы поиска:**\n",
    "*   `driver.find_element(By.ЛОКАТОР, 'значение')`: Ищет **первый** элемент.\n",
    "*   `driver.find_elements(By.ЛОКАТОР, 'значение')`: Ищет **все** элементы и возвращает список.\n",
    "\n",
    "**Методы взаимодействия:**\n",
    "*   `.click()`: Кликнуть по элементу.\n",
    "*   `.send_keys('текст')`: Ввести текст в поле ввода.\n",
    "*   `.text`: Получить видимый текст элемента.\n",
    "*   `.get_attribute('атрибут')`: Получить значение атрибута (например, `href`).\n",
    "\n",
    "**Пример:**\n",
    "```python\n",
    "# Найти поле поиска по его ID\n",
    "search_box = driver.find_element(By.ID, 'search-input')\n",
    "\n",
    "# Ввести текст в поле\n",
    "search_box.send_keys('Погода в Санкт-Петербурге')\n",
    "\n",
    "# Найти и кликнуть по кнопке поиска\n",
    "search_button = driver.find_element(By.CLASS_NAME, 'search-button')\n",
    "search_button.click()\n",
    "```\n",
    "\n",
    "##### **2.5. Интеграция с BeautifulSoup и завершение работы**\n",
    "\n",
    "После того как Selenium выполнил все необходимые действия (клики, прокрутку) и дождался появления данных, мы можем получить итоговый HTML-код страницы.\n",
    "\n",
    "*   `driver.page_source`: Атрибут, содержащий финальный HTML-код страницы в виде строки.\n",
    "\n",
    "Этот код можно передать в `BeautifulSoup` для более удобного и быстрого парсинга, комбинируя сильные стороны обеих библиотек.\n",
    "\n",
    "**Обязательный шаг: Завершение сессии**\n",
    "После окончания работы необходимо закрыть браузер и завершить сессию WebDriver, чтобы освободить системные ресурсы.\n",
    "\n",
    "*   `driver.quit()`: Закрывает все окна браузера и завершает процесс WebDriver.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuF-Ty7YmGCg"
   },
   "source": [
    "### Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "RUxZBQrWLKEi",
    "outputId": "a54ca288-859c-4227-a4bf-c90f49be5b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отправляю запрос на: https://api.github.com/users/gvanrossum/repos\n",
      "Запрос успешен!\n",
      "Последние 10 созданных репозиториев пользователя gvanrossum:\n",
      "  - Название: TypeChat, URL: https://github.com/gvanrossum/TypeChat\n",
      "  - Название: devguide, URL: https://github.com/gvanrossum/devguide\n",
      "  - Название: c-parser, URL: https://github.com/gvanrossum/c-parser\n",
      "  - Название: old-demos, URL: https://github.com/gvanrossum/old-demos\n",
      "  - Название: minithesis, URL: https://github.com/gvanrossum/minithesis\n",
      "  - Название: exceptiongroup, URL: https://github.com/gvanrossum/exceptiongroup\n",
      "  - Название: http-get-perf, URL: https://github.com/gvanrossum/http-get-perf\n",
      "  - Название: pythonlabs-com-azure, URL: https://github.com/gvanrossum/pythonlabs-com-azure\n",
      "  - Название: peps, URL: https://github.com/gvanrossum/peps\n",
      "  - Название: patma, URL: https://github.com/gvanrossum/patma\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json # Библиотека для работы с JSON\n",
    "\n",
    "# --- ШАГ 1: Определение цели ---\n",
    "# URL API для получения репозиториев пользователя.\n",
    "# Обратите внимание, что это не обычный URL для браузера!\n",
    "api_base_url = \"https://api.github.com/users/\"\n",
    "username = \"gvanrossum\"\n",
    "full_api_url = f\"{api_base_url}{username}/repos\"\n",
    "\n",
    "# --- ШАГ 2: Использование параметров запроса (params) ---\n",
    "# API позволяет настраивать вывод. Мы хотим отсортировать репозитории\n",
    "# по дате создания ('created') и получать по 10 штук за раз.\n",
    "# Для этого используются GET-параметры, которые requests умеет добавлять к URL.\n",
    "\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Создайте словарь 'params', который будет содержать следующие GET-параметры:\n",
    "# 1. 'sort': со значением 'created' (сортировка по дате создания)\n",
    "# 2. 'per_page': со значением '10' (выводить по 10 репозиториев на странице)\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "params = {\n",
    "    'sort': 'created',\n",
    "    'per_page': '10'\n",
    "}\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "\n",
    "# --- ШАГ 3: Выполнение запроса и обработка JSON ---\n",
    "print(f\"Отправляю запрос на: {full_api_url}\")\n",
    "response = requests.get(full_api_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Запрос успешен!\")\n",
    "    # --- ЗАДАНИЕ ---\n",
    "    # Ответ от API приходит в формате JSON.\n",
    "    # У объекта response есть специальный метод .json(), который\n",
    "    # автоматически преобразует этот ответ в python-объект (список словарей).\n",
    "    # Используйте его и сохраните результат в переменную 'repos_data'.\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    repos_data = response.json()\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    # --- ШАГ 4: Вывод результатов ---\n",
    "    print(f\"Последние 10 созданных репозиториев пользователя {username}:\")\n",
    "    # Пройдемся циклом по списку репозиториев и выведем их названия и URL\n",
    "    for repo in repos_data:\n",
    "        print(f\"  - Название: {repo['name']}, URL: {repo['html_url']}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Ошибка! Статус-код: {response.status_code}\")\n",
    "    print(f\"Сообщение: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "jGRj_FXxLMwq",
    "outputId": "74b446f3-88bd-4552-95bd-2aadf23a4cce"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Логотип находится в теге <img> внутри тега <a> с href=\"/\".\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# --- ЗАДАНИЕ ---\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Найдите тег 'img' и извлеките из него значение атрибута 'src'.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Сохраните относительный URL в переменную 'relative_logo_url'.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\u001b[39;00m\n\u001b[0;32m     17\u001b[0m logo_element \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, href\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m relative_logo_url \u001b[38;5;241m=\u001b[39m logo_element[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# --- ШАГ 2: Преобразование относительного URL в абсолютный ---\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Ссылка в 'src' относительная ('/images/logo.png').\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Чтобы ее скачать, нужен полный URL.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m absolute_logo_url \u001b[38;5;241m=\u001b[39m urljoin(base_url, relative_logo_url)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import Image # Для отображения картинки в Colab\n",
    "\n",
    "# --- ШАГ 1: Найти URL изображения ---\n",
    "# Сначала нам нужно, как в прошлой лабораторной, найти ссылку на логотип.\n",
    "base_url = 'http://quotes.toscrape.com/'\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Логотип находится в теге <img> внутри тега <a> с href=\"/\".\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Найдите тег 'img' и извлеките из него значение атрибута 'src'.\n",
    "# Сохраните относительный URL в переменную 'relative_logo_url'.\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "logo_element = soup.find('a', href='/').find('img')\n",
    "relative_logo_url = logo_element['src']\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "# --- ШАГ 2: Преобразование относительного URL в абсолютный ---\n",
    "# Ссылка в 'src' относительная ('/images/logo.png').\n",
    "# Чтобы ее скачать, нужен полный URL.\n",
    "absolute_logo_url = urljoin(base_url, relative_logo_url)\n",
    "print(f\"Найден абсолютный URL логотипа: {absolute_logo_url}\")\n",
    "\n",
    "\n",
    "# --- ШАГ 3: Скачивание бинарного контента ---\n",
    "print(\"Скачиваю изображение...\")\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Отправьте GET-запрос на 'absolute_logo_url'.\n",
    "# Ответ для бинарных файлов нужно получать через атрибут .content, а не .text\n",
    "# Сохраните результат в переменную 'image_content'.\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "image_response = requests.get('absolute_logo_url')\n",
    "if image_response.status_code == 200:\n",
    "    image_content = image_response.content\n",
    "    print(\"Изображение успешно скачано!\")\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    # --- ШАГ 4: Сохранение файла на диск ---\n",
    "    # Используем стандартный синтаксис Python для записи файлов.\n",
    "    # 'wb' означает \"write binary\" - запись в бинарном режиме.\n",
    "    file_name = 'logo.png'\n",
    "    # --- ЗАДАНИЕ ---\n",
    "    # Откройте файл 'file_name' для записи в бинарном режиме ('wb')\n",
    "    # и запишите в него 'image_content'.\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(image_content)\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "    print(f\"Файл '{file_name}' сохранен в текущую директорию Colab.\")\n",
    "\n",
    "    # Отобразим скачанное изображение прямо в блокноте\n",
    "    display(Image(file_name))\n",
    "else:\n",
    "    print(f\"Ошибка при скачивании изображения! Статус-код: {image_response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U80X12wQLOwz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Получен CSRF токен: qPWCOdDkUxzRILetKTrhbHpngVMjNZGlyQfvmFoisYJBacAXuEwS\n",
      "\n",
      "Пробую получить доступ к главной странице после логина...\n",
      "Успех! Мы авторизованы. Сервер видит кнопку 'Logout'.\n",
      "Logout\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- ШАГ 1: Создание сессии и получение CSRF-токена ---\n",
    "# requests.Session() - это объект, который будет \"помнить\" cookies между запросами.\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Создайте объект сессии и сохраните его в переменную 'session'.\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "session = requests.Session()\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "login_url = 'http://quotes.toscrape.com/login'\n",
    "\n",
    "# Сначала делаем GET-запрос, чтобы получить страницу входа и специальный\n",
    "# \"csrf_token\" - это защита от межсайтовой подделки запроса.\n",
    "response_login_page = session.get(login_url)\n",
    "soup_login = BeautifulSoup(response_login_page.text, 'html.parser')\n",
    "\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Найдите тег 'input' у которого атрибут name равен 'csrf_token'\n",
    "# и извлеките из него значение атрибута 'value'.\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "csrf_token = soup_login.find('input', {'name': 'csrf_token'})['value']\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "print(f\"Получен CSRF токен: {csrf_token}\")\n",
    "\n",
    "\n",
    "# --- ШАГ 2: Подготовка данных для POST-запроса ---\n",
    "# Это данные, которые мы бы ввели в форму на сайте.\n",
    "# Имена полей ('username', 'password') нужно посмотреть в HTML-коде страницы.\n",
    "payload = {\n",
    "    'csrf_token': csrf_token,\n",
    "    'username': 'admin',  # Используем стандартные учетные данные для этого сайта\n",
    "    'password': 'admin'\n",
    "}\n",
    "\n",
    "\n",
    "# --- ШАГ 3: Отправка POST-запроса для аутентификации ---\n",
    "# Мы отправляем POST-запрос на тот же URL, но уже с нашими данными.\n",
    "# Сессия автоматически сохранит cookies, которые вернет сервер после успешного входа.\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Отправьте POST-запрос с помощью объекта 'session'.\n",
    "# URL: login_url\n",
    "# Данные: payload\n",
    "# Сохраните ответ в 'response_post'.\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "response_post = session.post(login_url, data=payload)\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "\n",
    "# --- ШАГ 4: Доступ к защищенной странице ---\n",
    "# Теперь, используя ту же сессию, мы можем зайти на любую страницу сайта,\n",
    "# и сервер будет \"видеть\" нас как залогиненного пользователя.\n",
    "print(\"\\nПробую получить доступ к главной странице после логина...\")\n",
    "response_main_page = session.get('http://quotes.toscrape.com/')\n",
    "soup_main = BeautifulSoup(response_main_page.text, 'html.parser')\n",
    "\n",
    "# Проверим, видим ли мы кнопку \"Logout\"\n",
    "logout_button = soup_main.find('a', href='/logout')\n",
    "\n",
    "if logout_button:\n",
    "    print(\"Успех! Мы авторизованы. Сервер видит кнопку 'Logout'.\")\n",
    "    print(logout_button.text)\n",
    "else:\n",
    "    print(\"Неудача. Авторизация не удалась, кнопка 'Logout' не найдена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjoG6qGmmJkG"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2FxGZ6QmLdA"
   },
   "source": [
    "### BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BBNhpZgPMGUv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено книг на странице: 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>Â£51.77</td>\n",
       "      <td>Three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>Â£53.74</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>Â£50.10</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>Â£47.82</td>\n",
       "      <td>Four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>Â£54.23</td>\n",
       "      <td>Five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>Â£22.65</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>Â£33.34</td>\n",
       "      <td>Four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>Â£17.93</td>\n",
       "      <td>Three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>Â£22.60</td>\n",
       "      <td>Four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>Â£52.15</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>Â£13.99</td>\n",
       "      <td>Two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>Â£20.66</td>\n",
       "      <td>Four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>Â£17.46</td>\n",
       "      <td>Five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>Â£52.29</td>\n",
       "      <td>Five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>Â£35.02</td>\n",
       "      <td>Five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>Â£57.25</td>\n",
       "      <td>Three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>Â£23.88</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>Â£37.59</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>Â£51.33</td>\n",
       "      <td>Two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>Â£45.17</td>\n",
       "      <td>Two</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title    Price Rating\n",
       "0                                A Light in the Attic  Â£51.77  Three\n",
       "1                                  Tipping the Velvet  Â£53.74    One\n",
       "2                                          Soumission  Â£50.10    One\n",
       "3                                       Sharp Objects  Â£47.82   Four\n",
       "4               Sapiens: A Brief History of Humankind  Â£54.23   Five\n",
       "5                                     The Requiem Red  Â£22.65    One\n",
       "6   The Dirty Little Secrets of Getting Your Dream...  Â£33.34   Four\n",
       "7   The Coming Woman: A Novel Based on the Life of...  Â£17.93  Three\n",
       "8   The Boys in the Boat: Nine Americans and Their...  Â£22.60   Four\n",
       "9                                     The Black Maria  Â£52.15    One\n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)  Â£13.99    Two\n",
       "11                              Shakespeare's Sonnets  Â£20.66   Four\n",
       "12                                        Set Me Free  Â£17.46   Five\n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...  Â£52.29   Five\n",
       "14                          Rip it Up and Start Again  Â£35.02   Five\n",
       "15  Our Band Could Be Your Life: Scenes from the A...  Â£57.25  Three\n",
       "16                                               Olio  Â£23.88    One\n",
       "17  Mesaerion: The Best Science Fiction Stories 18...  Â£37.59    One\n",
       "18                       Libertarianism for Beginners  Â£51.33    Two\n",
       "19                            It's Only the Himalayas  Â£45.17    Two"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# --- ШАГ 1: Получение HTML ---\n",
    "url = 'http://books.toscrape.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# --- ШАГ 2: Поиск общего контейнера ---\n",
    "# Все книги находятся внутри элементов <article> с классом 'product_pod'.\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Найдите ВСЕ такие контейнеры с помощью .find_all() и сохраните в 'all_books'.\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "all_books = soup.find_all('article', class_='product_pod')\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "print(f\"Найдено книг на странице: {len(all_books)}\")\n",
    "books_data = []\n",
    "\n",
    "# --- ШАГ 3: Извлечение данных в цикле ---\n",
    "for book in all_books:\n",
    "    # --- ЗАДАНИЕ A: Найти название книги ---\n",
    "    # Название находится в теге <a> внутри тега <h3>.\n",
    "    # Нужно извлечь его атрибут 'title'.\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    title = book.find('h3').find('a')['title']\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    # --- ЗАДАНИЕ B: Найти цену ---\n",
    "    # Цена находится в теге <p> с классом 'price_color'.\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    price = book.find('p', class_='price_color').text\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    # --- ЗАДАНИЕ C: Найти рейтинг ---\n",
    "    # Рейтинг находится в атрибуте 'class' у тега <p>, который начинается с 'star-rating'.\n",
    "    # Например: <p class=\"star-rating Three\">. Нам нужно извлечь слово 'Three'.\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    rating_container = book.find('p', class_='star-rating')\n",
    "    # Атрибут 'class' возвращает список классов, например, ['star-rating', 'Three']\n",
    "    rating = rating_container['class'][1] # Берем второй элемент\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    # Добавляем собранные данные в наш список\n",
    "    books_data.append({\n",
    "        'Title': title,\n",
    "        'Price': price,\n",
    "        'Rating': rating\n",
    "    })\n",
    "\n",
    "# --- ШАГ 4: Вывод результата ---\n",
    "df = pd.DataFrame(books_data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "3yh3mDG1MJPR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Характеристика</th>\n",
       "      <th>Значение</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Семантика</td>\n",
       "      <td>Императивное, процедурное, структурное програм...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Класс языка</td>\n",
       "      <td>Мультипарадигменный язык программирования</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Тип исполнения</td>\n",
       "      <td>Интерпретируемый</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Появился в</td>\n",
       "      <td>20 февраля 1991[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Автор</td>\n",
       "      <td>Гвидо ван Россум[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Разработчик</td>\n",
       "      <td>Python Software Foundation и Гвидо ван Россум[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Расширение файлов</td>\n",
       "      <td>.py, .pyc, .pyo (до версии 3.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Выпуск</td>\n",
       "      <td>3.13.7 (14 августа 2025)[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Испытал влияние</td>\n",
       "      <td>ABC[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Лицензия</td>\n",
       "      <td>Python Software Foundation License[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Сайт</td>\n",
       "      <td>python.org (англ.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ОС</td>\n",
       "      <td>кроссплатформенность[7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Характеристика                                           Значение\n",
       "0           Семантика  Императивное, процедурное, структурное програм...\n",
       "1         Класс языка          Мультипарадигменный язык программирования\n",
       "2      Тип исполнения                                   Интерпретируемый\n",
       "3          Появился в                                 20 февраля 1991[4]\n",
       "4               Автор                                Гвидо ван Россум[4]\n",
       "5         Разработчик   Python Software Foundation и Гвидо ван Россум[4]\n",
       "6   Расширение файлов                    .py, .pyc, .pyo (до версии 3.5)\n",
       "7              Выпуск                        3.13.7 (14 августа 2025)[5]\n",
       "8     Испытал влияние                                             ABC[6]\n",
       "9            Лицензия              Python Software Foundation License[4]\n",
       "10               Сайт                                 python.org (англ.)\n",
       "11                 ОС                            кроссплатформенность[7]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL страницы книги: http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
      "Сырая строка с ценой: Â£51.77\n",
      "Очищенная цена (число): 51.77\n",
      "Тип данных цены: <class 'float'>\n",
      "\n",
      "Сырое описание:\n",
      " It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20t...\n",
      "\n",
      "Очищенное описание:\n",
      " It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20t...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# --- ШАГ 1: Получение HTML ---\n",
    "url = 'https://ru.wikipedia.org/wiki/Python'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# --- ШАГ 2: Поиск таблицы-инфобокса ---\n",
    "# Инфобокс - это таблица с классом 'infobox'.\n",
    "infobox = soup.find('table')\n",
    "\n",
    "# --- ШАГ 3: Поиск всех строк с данными ---\n",
    "# Все строки в таблице - это теги <tr>.\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Найдите все теги <tr> внутри 'infobox'.\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "rows = infobox.find_all('tr')\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "infobox_data = []\n",
    "\n",
    "# --- ШАГ 4: Навигация по соседним элементам в цикле ---\n",
    "for row in rows:\n",
    "    # Нас интересуют строки, где есть и заголовок (<th>) и значение (<td>)\n",
    "    header = row.find('th')\n",
    "    data_cell = row.find('td')\n",
    "\n",
    "    if header and data_cell:\n",
    "        # --- ЗАДАНИЕ ---\n",
    "        # Извлеките текст из ячейки-заголовка 'header'.\n",
    "        # Извлеките текст из ячейки-данных 'data_cell'.\n",
    "        # Используйте .text.strip() для удаления лишних пробелов и переносов строк.\n",
    "        # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "        key = header.text.strip()\n",
    "        value = data_cell.text.strip()\n",
    "        # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "        infobox_data.append({\n",
    "            'Характеристика': key,\n",
    "            'Значение': value\n",
    "        })\n",
    "\n",
    "# --- ШАГ 5: Вывод результата ---\n",
    "df = pd.DataFrame(infobox_data)\n",
    "display(df)\n",
    "'''**Чему вы научились:**\n",
    "1.  Искать вложенные элементы (`infobox.find_all('tr')`).\n",
    "2.  **Навигации по горизонтали:** находить \"соседние\" теги (`<th>` и `<td>`) внутри общего родителя (`<tr>`).\n",
    "3.  Использовать `.strip()` для базовой очистки текстовых данных.\n",
    "\n",
    "---\n",
    "### **Задача 3: \"Дата-клинер\" — Очистка и преобразование данных**\n",
    "\n",
    "**Контекст:** Реальные данные почти всегда \"грязные\". Цены содержат символы валют, текст - лишние пробелы и переносы, а числа хранятся в виде строк. Профессиональный парсинг включает в себя не только извлечение, но и очистку.\n",
    "\n",
    "**Ваша миссия:** Перейти на страницу первой книги из каталога, извлечь из нее цену и описание продукта, а затем \"очистить\" их: цену превратить в число, а из описания убрать лишние символы.\n",
    "\n",
    "**(Ячейка 3: Код)**\n",
    "python'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re # Библиотека для работы с регулярными выражениями\n",
    "\n",
    "# --- ШАГ 1: Получаем URL первой книги ---\n",
    "base_url = 'http://books.toscrape.com/' # Важно: для urljoin нужен правильный базовый путь\n",
    "main_page_url = 'http://books.toscrape.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'}\n",
    "response = requests.get(main_page_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Находим ссылку на первую книгу\n",
    "first_book_link = soup.find('h3').find('a')['href']\n",
    "book_url = urljoin(base_url, first_book_link)\n",
    "print(f\"URL страницы книги: {book_url}\")\n",
    "\n",
    "# --- ШАГ 2: Переходим на страницу книги и создаем новый \"суп\" ---\n",
    "response_book = requests.get(book_url, headers=headers)\n",
    "soup_book = BeautifulSoup(response_book.text, 'html.parser')\n",
    "\n",
    "\n",
    "# --- ШАГ 3: Извлечение и очистка ЦЕНЫ ---\n",
    "price_raw = soup_book.find('p', class_='price_color').text\n",
    "print(f\"Сырая строка с ценой: {price_raw}\") # Например, '£51.77'\n",
    "\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Напишите код для очистки цены.\n",
    "# Нужно убрать все символы, кроме цифр и точки, а затем превратить строку в число (float).\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "# re.sub(r'[^\\d.]', '', price_raw) - эта команда удалит все, что не является цифрой (\\d) или точкой (.).\n",
    "price_clean_str = re.sub(r'[^\\d.]', '', price_raw)\n",
    "price_float = float(price_clean_str)\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "print(f\"Очищенная цена (число): {price_float}\")\n",
    "print(f\"Тип данных цены: {type(price_float)}\")\n",
    "\n",
    "\n",
    "# --- ШАГ 4: Извлечение и очистка ОПИСАНИЯ ---\n",
    "# Описание находится в теге <p> сразу ПОСЛЕ тега <div> с id='product_description'.\n",
    "# Это идеальный случай для использования .find_next_sibling()\n",
    "# --- ЗАДАНИЕ ---\n",
    "# Найдите тег <div> с id 'product_description', а затем\n",
    "# с помощью .find_next_sibling('p') найдите следующий за ним тег <p>.\n",
    "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "description_header = soup_book.find('div', id='product_description')\n",
    "description_raw = description_header.find_next_sibling('p').text\n",
    "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "print(f\"\\nСырое описание:\\n {description_raw[:150]}...\")\n",
    "\n",
    "# Очищаем описание от лишних пробелов и символов\n",
    "description_clean = description_raw.strip()\n",
    "print(f\"\\nОчищенное описание:\\n {description_clean[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmYV2Vwj7hIi"
   },
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvMnn8-LmPtw"
   },
   "source": [
    "### Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RER97g1NSEX"
   },
   "source": [
    "### Настройка selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rU4wrDXFNUCd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (4.35.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\1032242353\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Установка Selenium и webdriver-manager для автоматического управления драйверами\n",
    "!pip install selenium webdriver-manager\n",
    "\n",
    "# Настройка опций для запуска Chrome в \"безголовом\" режиме (без GUI) в среде Colab\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- Этот код нужно будет использовать в каждой задаче для инициализации драйвера ---\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless') # Запуск без открытия окна браузера\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "# -----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OZgkTcm-NW3v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перешел на страницу: http://quotes.toscrape.com/scroll\n",
      "Достигнут конец страницы. Выхожу из цикла.\n",
      "\n",
      "Собрано уникальных цитат: 100\n",
      "Пример одной из собранных цитат:\n",
      "“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "\n",
    "# --- ШАГ 1: Инициализация драйвера и переход на страницу ---\n",
    "driver = setup_driver()\n",
    "url = 'http://quotes.toscrape.com/scroll'\n",
    "driver.get(url)\n",
    "print(f\"Перешел на страницу: {url}\")\n",
    "\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "# --- ШАГ 2: Логика \"бесконечной\" прокрутки ---\n",
    "# Мы будем прокручивать страницу вниз, пока количество цитат не перестанет увеличиваться.\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "all_quotes_texts = set() # Используем set для автоматического удаления дубликатов\n",
    "\n",
    "\n",
    "while True:\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"quote\")))\n",
    "    # --- ЗАДАНИЕ A: Собрать цитаты, видимые на данный момент ---\n",
    "    # Найдите все элементы с классом 'quote' и добавьте их текст в 'all_quotes_texts'.\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    quotes_elements = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "    for quote in quotes_elements:\n",
    "        text_element = quote.find_element(By.CLASS_NAME, \"text\")\n",
    "        quote_text = text_element.text\n",
    "        all_quotes_texts.add(quote_text)\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    # --- ЗАДАНИЕ B: Прокрутить страницу до самого низа ---\n",
    "    # Используйте driver.execute_script() для выполнения JavaScript-кода.\n",
    "    # Команда \"window.scrollTo(0, document.body.scrollHeight);\" прокручивает страницу вниз.\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    # Даем странице время на подгрузку нового контента\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # --- ШАГ 3: Проверка условия выхода из цикла ---\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        print(\"Достигнут конец страницы. Выхожу из цикла.\")\n",
    "        break # Выходим, если высота страницы больше не меняется\n",
    "    last_height = new_height\n",
    "\n",
    "# --- ШАГ 4: Вывод результата и закрытие браузера ---\n",
    "print(f\"\\nСобрано уникальных цитат: {len(all_quotes_texts)}\")\n",
    "print(\"Пример одной из собранных цитат:\")\n",
    "print(list(all_quotes_texts)[0])\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "viUnYMXmNZwT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перешел на страницу: https://www.aviasales.ru/\n",
      "Нажал 'Найти'. Жду появления результатов...\n",
      "Результаты загружены!\n",
      "Цена первого найденного билета: 9 650 ₽\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# --- ШАГ 1: Инициализация и переход на сайт ---\n",
    "driver = setup_driver()\n",
    "# ВАЖНО: Структура и селекторы на живых сайтах могут меняться!\n",
    "# Этот код актуален на момент написания.\n",
    "url = 'https://www.aviasales.ru/'\n",
    "driver.get(url)\n",
    "print(f\"Перешел на страницу: {url}\")\n",
    "time.sleep(2) # Небольшая пауза, чтобы страница прогрузилась\n",
    "\n",
    "# --- ШАГ 2: Ввод данных в форму поиска ---\n",
    "try:\n",
    "    # --- ЗАДАНИЕ A: Ввести город отправления ---\n",
    "    # Найдите поле ввода для города отправления (id='origin').\n",
    "    # Очистите его (.clear()) и введите \"Москва\" (.send_keys()).\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    origin = driver.find_element(By.ID, 'avia_form_origin-input')\n",
    "    origin.clear()\n",
    "    origin.send_keys(\"Москва\")\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "    time.sleep(1) # Пауза, чтобы появился выпадающий список\n",
    "\n",
    "    # --- ЗАДАНИЕ B: Ввести город назначения ---\n",
    "    # По аналогии найдите поле назначения (id='destination') и введите \"Санкт-Петербург\".\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    destination = driver.find_element(By.ID, 'avia_form_destination-input')\n",
    "    destination.send_keys(\"Санкт-Петербург\")\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "    time.sleep(1)\n",
    "\n",
    "    # --- ЗАДАНИЕ C: Нажать на кнопку поиска ---\n",
    "    # Кнопка поиска часто имеет сложный селектор.\n",
    "    # Будем искать по тексту \"Найти билеты\" внутри тега <div>\n",
    "    # XPath-селектор для этого: \"//div[text()='Найти билеты']\"\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    search_button = driver.find_element(By.XPATH, \"//div[text()='Найти билеты']\")\n",
    "    driver.execute_script(\"arguments[0].click();\", search_button)\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    # --- ШАГ 3: Ожидание и извлечение результатов ---\n",
    "    print(\"Нажал 'Найти'. Жду появления результатов...\")\n",
    "    # --- ЗАДАНИЕ D: Дождаться загрузки первого билета ---\n",
    "    # Это ключевой шаг. Мы ждем до 20 секунд, пока на странице не появится\n",
    "    # элемент, сигнализирующий о загрузке результатов (например, контейнер с ценой).\n",
    "    # Используем WebDriverWait и Expected Conditions (EC).\n",
    "    # Ищем первый элемент с атрибутом 'data-test-id=\"ticket-card\"'.\n",
    "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    first_ticket = wait.until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"[class='s__z11zsqgp5goZAXVc']\"))\n",
    "    )\n",
    "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
    "\n",
    "    print(\"Результаты загружены!\")\n",
    "    # Извлекаем цену из первого найденного билета\n",
    "    price = first_ticket.find_element(By.CSS_SELECTOR, \"[data-test-id='price']\").text\n",
    "    print(f\"Цена первого найденного билета: {price}\")\n",
    "\n",
    "finally:\n",
    "    # --- ШАГ 4: Обязательное закрытие браузера ---\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MorSgT4YmN_A"
   },
   "source": [
    "**Контрольные вопросы по библиотекам для парсинга**\n",
    "\n",
    "#### **Блок 1: Библиотека `requests` (\"Курьер\")**\n",
    "\n",
    "Эти вопросы проверяют ваше умение получать данные с сервера.\n",
    "\n",
    "1.  **Фундаментальный вопрос:** В чем основная задача библиотеки `requests`? Что она делает, а чего, наоборот, делать **не умеет** (например, в контексте JavaScript)?\n",
    "2.  **Типы запросов:** В чем принципиальная разница между `GET` и `POST` запросами? В какой из наших лабораторных задач мы использовали `POST` и для какой цели?\n",
    "3.  **Объект ответа:** Вы выполнили команду `response = requests.get(url)`. Какие три важнейших атрибута объекта `response` вы будете использовать и для чего каждый из них предназначен (`status_code`, `text`, `content`)?\n",
    "4.  **Обработка ошибок:** Почему проверка `response.status_code == 200` является обязательным шагом в любом надежном парсере? Что означает код `404`? А код `403`?\n",
    "5.  **Работа с API:** Почему для получения данных от API (как в задаче с GitHub) мы использовали метод `response.json()`, а не просто брали `response.text`? В чем преимущество такого подхода?\n",
    "6.  **Сессии:** Объясните своими словами, что такое `requests.Session()`. Какую проблему решает объект сессии, и почему без него не удалось бы выполнить задачу с авторизацией на сайте?\n",
    "7.  **Параметры запроса:** Как с помощью `requests` передать в URL GET-параметры (например, `?sort=date&page=2`) без ручного формирования строки URL? Какой аргумент функции `get()` для этого используется?\n",
    "\n",
    "---\n",
    "\n",
    "#### **Блок 2: Библиотека `BeautifulSoup` (\"Навигатор\")**\n",
    "\n",
    "Эти вопросы проверяют ваше умение разбирать HTML-код и находить в нем нужные данные.\n",
    "\n",
    "8.  **Основное назначение:** Какую проблему решает `BeautifulSoup`? Что она принимает на вход и что отдает на выходе?\n",
    "9.  **Ключевое различие:** В чем фундаментальная разница между методами `.find()` и `.find_all()`? Приведите пример, когда нужно использовать один, а когда — другой.\n",
    "10. **Синтаксис поиска:** Как найти тег `<p>` с CSS-классом `price_color`? Почему в коде мы пишем `class_` с нижним подчеркиванием, а не просто `class`?\n",
    "11. **Извлечение данных:** У вас есть объект тега, сохраненный в переменной `tag`. Как из него извлечь:\n",
    "    *   Весь видимый текст внутри него?\n",
    "    *   Значение атрибута `href`?\n",
    "12. **Вложенный поиск:** Ваш парсер нашел общий контейнер товара (`<div class=\"product\">`). Как продолжить поиск и найти цену, которая находится **внутри** этого контейнера? Напишите примерный код.\n",
    "13. **Продвинутая навигация:** Представьте, что вы нашли заголовок `<h2>Описание</h2>`. Само описание находится в следующем за ним теге `<p>`. Какой метод `BeautifulSoup` позволит вам найти этот \"соседний\" тег, не начиная поиск заново от корня документа?\n",
    "14. **Очистка данных:** Почему простого извлечения `.text` часто недостаточно для реальных задач? Какие две стандартные операции по очистке текста вы применяли в лабораторных работах?\n",
    "\n",
    "---\n",
    "\n",
    "#### **Блок 3: Библиотека `Selenium` (\"Робот-пользователь\")**\n",
    "\n",
    "Эти вопросы проверяют ваше понимание работы с динамическими сайтами и автоматизацией браузера.\n",
    "\n",
    "15. **Главный вопрос:** Назовите основную причину, по которой мы вынуждены использовать `Selenium`, а не `requests`. Какую технологию `Selenium` умеет обрабатывать, а `requests` — нет?\n",
    "16. **Проблема синхронизации:** Почему использование `time.sleep()` для ожидания загрузки элементов на странице является плохой практикой? Каков правильный, надежный способ дождаться появления элемента?\n",
    "17. **Явные ожидания (Explicit Waits):** Объясните своими словами, что делают эти три строки кода:\n",
    "    ```python\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    element = wait.until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'price'))\n",
    "    )\n",
    "    ```\n",
    "18. **Взаимодействие с формами:** Опишите последовательность из трех основных действий, которые нужно совершить с помощью `Selenium`, чтобы ввести текст в поле поиска и нажать на кнопку.\n",
    "19. **Исполнение JavaScript:** Для чего в задаче с \"бесконечным свитком\" мы использовали команду `driver.execute_script()`? Можно ли было добиться того же результата другим методом `Selenium`?\n",
    "20. **Интеграция библиотек:** В какой момент работы парсера на `Selenium` имеет смысл передать управление библиотеке `BeautifulSoup`? Что для этого нужно получить от `driver` и как это сделать?\n",
    "21. **Завершение работы:** Почему команда `driver.quit()` является обязательной в конце скрипта? Что произойдет, если ее не вызывать?\n",
    "\n",
    "---\n",
    "\n",
    "#### **Блок 4: Синтез и сценарии (Проверка общего понимания)**\n",
    "\n",
    "22. **Выбор инструмента:** Вам нужно спарсить три сайта:\n",
    "    *   А) Таблицу курсов валют со страницы Центробанка.\n",
    "    *   Б) Ленту комментариев на YouTube, которая подгружается при прокрутке.\n",
    "    *   В) Данные о погоде с публичного погодного API.\n",
    "    Какой основной инструмент (`requests`, `bs4`, `Selenium`) вы выберете для **каждой** из этих задач и почему?\n",
    "\n",
    "23. **Отладка:** Ваш парсер на `BeautifulSoup` вчера работал, а сегодня перестал, выдавая ошибку `AttributeError: 'NoneType' object has no attribute 'text'`. Назовите самую вероятную причину этой проблемы. Каков ваш первый шаг для диагностики?\n",
    "\n",
    "24. **Этика парсинга:** Что такое файл `robots.txt` на сайте и почему его рекомендуется проверять перед запуском массового сбора данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено таблиц: 7\n",
      "['infobox', 'infobox-9dc5a174604cb023']\n",
      "['wikitable']\n",
      "['wikitable']\n",
      "['nowraplinks', 'collapsible', 'autocollapse', 'navbox-inner']\n",
      "['nowraplinks', 'hlist', 'collapsible', 'autocollapse', 'navbox-inner']\n",
      "['nowraplinks', 'hlist', 'navbox-inner']\n",
      "['nowraplinks', 'authoritycontrol', 'collapsible', 'collapsed', 'navbox-subgroup']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://ru.wikipedia.org/wiki/Python'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "tables = soup.find_all('table')\n",
    "print(f'Найдено таблиц: {len(tables)}')\n",
    "for t in tables:\n",
    "    print(t.get('class'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
